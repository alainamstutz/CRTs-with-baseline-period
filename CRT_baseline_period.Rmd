---
title: "CRT_baseline_period"
author: "A.Amstutz"
date: "2025-03-15"
output:
  html_document:
    keep_md: yes
    toc: yes
    toc_float: yes
    code_folding: hide
  pdf_document:
    toc: yes
---

## Parallel CRT with baseline period
1. "A common enhancement of a simple parallel CRT is to add an assessment of participantsâ€™ outcomes in a baseline period (before randomisation). Even if different participants are assessed at baseline and follow-up [i.e. cross-sectional sampling], the fact that they are sampled from the same cluster allows some control for cluster differences." -> https://www.bmj.com/content/360/bmj.k1121
2. This is illustratively shown in the sample size calculator: https://clusterrcts.shinyapps.io/rshinyapp/ (switch between "Parallel" and "Parallel with baseline measure") -> can yield a substantial increase in power! See last chapter below.

Let's explore this further
* The rationale: The more variability there is in the outcome across clusters, the more difficult to identify the effect.
* On the example of an individual RCT: "if we are interested in measuring the impact of an intervention on the quality of life (QOL) across a diverse range of patients, the measurement (which typically ranges from 0 to 1) might vary considerably from person to person, regardless of the intervention. If the intervention has a real but moderate effect of, say, 0.1 points, it could easily get lost if the standard deviation is considerably larger, say 0.25."
* If we collect baseline QOL scores and can â€œcontrolâ€ for those measurements in some way (by conducting a repeated measures analysis, using ANCOVA, or assessing the difference itself as an outcome), we might be able to reduce the variability and have better chance to pick up the effect. 

Literature:
* https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5352 
* Hooper Richard et al. Sample size calculation for stepped wedge and other longitudinal cluster randomised trials. Statistics in Medicine. 2016: https://onlinelibrary.wiley.com/doi/10.1002/sim.7028 and 
* Leyrat ClÃ©mence et al. Practical considerations for sample size calculation for cluster randomized trials. Journal of Epidemiology and Population Health. 2024: https://www.sciencedirect.com/science/article/pii/S2950433324000090
* Based on: https://www.bmj.com/content/360/bmj.k1121.long

## There are various ways to do it:
1. Analysis of covariance (ANCOVA): Aggregate outcomes at baseline, and adjusts each individual participant at follow-up for the baseline cluster mean
2. Constrained baseline analysis: Treat outcomes collected at baseline and follow-up as longitudinal, and to use a repeated measures analysis to estimate the effect of the intervention being switched on in one of the randomised groups on the second of these occasions, see design matrix in https://clusterrcts.shinyapps.io/rshinyapp/. Unlike a difference of differences analysis, it assumes that there is no systematic difference between the groups at baseline.

## Simulation | Continuous outcome
### First, start with a simple individual randomized trial
```{r message=FALSE, warning=FALSE}
RNGkind("L'Ecuyer-CMRG")
set.seed(19287)
library(simstudy)
library(ggplot2)
library(lmerTest)
library(parallel)
library(data.table)
library(pwr)
library(gtsummary)
library(paletteer)
library(magrittr)
library(dplyr)
```

In this examples the overall variance isğœ= 64 (regardless of treatment group) and thus here, the individual-levelğœ= 64 is the only source of variation. The overall effect size ğ›¿, which is the difference in average QoL scores across treatment groups, is assumed to be 2.4, a standardized effect size 2.4/8=0.3, i.e., the treatment effect of 2.4 is about 0.3 standard deviations large (Cohen's d), which is a moderate effect size in typical clinical trials.
Let's calculate the sample size for a two-sample t-test (2 groups, independent, only 1 measurement at the end) based on the effect size (Cohen's d) and a power of 80%.
```{r}
pwr.t.test(d = 0.3, power = 0.80)
```
We will need 350 participants (175 in each arm) to achieve a power of 80%.

#### Let's generate the data
```{r}
simple_rct <- function(N) {
  
  # data definition for outcome
  
  defS <- defData(varname = "rx", formula = "1;1", dist = "trtAssign") # treatment variable, 1:1 allocation, 50% prob to be in a or b
  defS <- defData(defS, varname = "y", formula = "2.4*rx", variance = 64, dist = "normal") # outcome, continuous, 
  # If rx = 1 (treatment group), the outcome will be 2.4*1 = 2.4.
  # If rx = 0 (control group), the outcome will be 2.4*0 = 0.
  # variance: spread or variability in the scores within each group.
  # dist = "normal" indicates that the outcome variable (y) will follow a normal distribution.
  # The outcome variable has a mean of 2.4 for the treatment group and 0 for the control group, with a variance of 64.
  dd <- genData(N, defS)
  
  dd[]
}

dd <- simple_rct(350) # simulate the trial with 350 participants

ggplot(dd, aes(x = factor(rx), y = y, fill = factor(rx))) + 
  # Violin plot
  geom_violin(trim = FALSE) + 
  # Add the individual data points
  geom_jitter(width = 0.15, size = 1, alpha = 0.6, color = "black") +
  # Add a point estimate (median)
  stat_summary(fun = "median", geom = "point", shape = 18, size = 3, color = "red") +
  # Boxplot to show interquartile range (optional, can be removed if not needed)
  geom_boxplot(width = 0.1, alpha = 0.5, outlier.shape = NA, color = "black") +
  labs(x = "Treatment Group (rx)", y = "Outcome (y)") +
  scale_fill_manual(values = c("skyblue", "salmon")) + 
  theme_minimal() +
  theme(legend.title = element_blank()) +
  ggtitle("Violin Plots with Point Estimate and Individual Data Points")

```

#### Let's estimate the effect size
A simple linear regression model will do
```{r}
fit1 <- lm(y ~ rx, data = dd)
tbl_regression(fit1) %>% 
  modify_footnote(ci ~ NA, abbreviation = TRUE)
```

#### Let's confirm the power
We can confirm the power by repeatedly generating data sets and fitting models, recording the p-values for each replication.
```{r}
replicate <- function() {
  dd <- simple_rct(350)
  fit1 <- lm(y ~  rx, data = dd)
  coef(summary(fit1))["rx", "Pr(>|t|)"]
}

p_values <- mclapply(1:1000, function(x) replicate(), mc.cores = 4) # mclapply() is a parallelized version of the lapply() function that applies the replicate() function 1000 times. mc.cores = 4 means the code will use 4 cores for parallel computation, which speeds up the process by running multiple replications simultaneously.

# Estimated power based on 1000 replications.
# Convert the list of p-values into a vector and calculates the proportion of replications where the p-value is less than 0.05
mean(unlist(p_values) < 0.05)
```

### Second, now move to a cluster randomized trial, but same context
Parallel cluster randomized trial:
ğ‘Œğ‘–ğ‘—=ğ›¼+ğ›¿ğ‘ğ‘—+ğ‘ğ‘—+ğ‘ ğ‘–

where ğ‘Œğ‘–ğ‘— is a continuous outcome for participant ğ‘– in site ğ‘—. ğ‘ğ‘— is the treatment indicator for site ğ‘—. Again, ğ›¿ is the treatment effect. ğ‘ğ‘—âˆ¼ğ‘(0,ğœ^2ğ‘) is a site level effect, and ğ‘ ğ‘–âˆ¼ğ‘(0,ğœ^2ğ‘ ) is the participant level effect. The correlation of any two participants in a cluster is ğœŒ (the ICC):

ğœŒ=ğœ^2ğ‘/ (ğœ^2ğ‘+ğœ^2ğ‘ )

This tells us how correlated participants are within the same cluster.
If Ï is close to 0, most of the variability is at the individual level.
If Ï is close to 1, most of the variability is at the site level.

If we have a pre-specified number (ğ‘›) of participants at each site, we can estimate the sample size required in the CRT applying a design effect 1+(ğ‘›âˆ’1)ğœŒ to the sample size of an RCT that has the same overall variance.
We know the overall variance (ğœ^2) is 64 and we assume/know the ICC/p is 0.15. That brings us to (using the ICC formula above)

Ïƒ^2 c = 9.6 (site-level variance)
Ïƒ^2 s = 54.4 (individual-level variance)
ICC (Ï) = 0.15: 15% of the total variability in the outcome is due to differences between sites, while 85% is due to individual differences within sites.

Since individuals in the same cluster are correlated, we need to adjust the sample size using the design effect.
Design Effect = 1+(nâˆ’1)Ï; where, n=30 (number of individuals per site), Ï = 0.15
=> Design effect = 5.35
=> New Total Sample Size = 5.35 Ã— 350 = 1872
Since each site includes 30 participants, the number of required sites is: 62.4 => 64

#### Let's generate the data
```{r}
simple_crt <- function(nsites, n) {
  # treatment assignment, again, 1:1, 50% prob to be in each group
  defC <- defData(varname = "rx", formula = "1;1", dist = "trtAssign")
  # Define Cluster-Level Data (defC)
  # c (Random Site Effect from Normal Distribution)
  # This represents site-level variation. It follows a normal distribution
  # variance = 9.6 is the site-level variance (see above)
  defC <- defData(defC, varname = "c", formula = "0", variance = 9.6, dist = "normal")  
  # Define Individual-Level Outcome (defS)
  # c: Site-level effect (previously defined).
  # 2.4 * rx: Treatment effect for clusters assigned to intervention, see above
  # s_i \sim N(0, 54.4): Individual-level noise, with variance 54.4.
  defS <- defDataAdd(varname="y", formula="c + 2.4*rx", variance = 54.4, dist="normal")

# site/cluster level data
  # Generates nsites rows (one per cluster)
  # Each row includes: rx (treatment assignment for the site) and c (site-specific random effect).
  dc <- genData(nsites, defC, id = "site")

# individual level data
  # Creates individual-level data by assigning n individuals to each site.
  # Adds outcome (y) based on c, rx, and individual-level variance.
  dd <- genCluster(dc, "site", n, "id")
  dd <- addColumns(defS, dd)
  
  dd[]
}
# generating data for 20 sites, each with 50 individuals.
dd <- simple_crt(20, 50)

# Create violin plot with sites on x-axis and colors for treatment/control
ggplot(dd, aes(x = factor(site), y = y, fill = factor(rx))) +
  geom_violin(trim = FALSE, alpha = 0.5) +  # Violin plot with transparency
  geom_boxplot(width = 0.2, outlier.shape = NA, color = "black") +  # Boxplot for central tendency
  geom_jitter(shape = 16, position = position_jitter(0.2), alpha = 0.4) +  # Scatter individual points
  scale_fill_manual(values = c("blue", "red"), labels = c("Control", "Treatment")) +
  labs(x = "Site (Cluster)", y = "Outcome (y)", title = "Outcome Distribution by Site") +
  theme_minimal() +
  theme(legend.title = element_blank(), axis.text.x = element_text(angle = 90, hjust = 1))
```

#### Let's estimate the effect size
A mixed effects model is used to estimate the effect size.
```{r}
dd <- simple_crt(200,100)

fit2 <- lmer(y ~  rx + (1|site), data = dd)
tbl_regression(fit2, tidy_fun = broom.mixed::tidy)  %>% 
  modify_footnote(ci ~ NA, abbreviation = TRUE)
```

#### Let's confirm the power
Confirm power and go back to the initial assumption of 64 sites with 30 participants per site, which is a massive increase from total sample size 350 (individual RCT) to 1920 in a simple CRT
```{r}
replicate <- function() {
  dd <- simple_crt(64, 30)
  fit2 <- lmer(y ~  rx + (1|site), data = dd)
  
  coef(summary(fit2))["rx", "Pr(>|t|)"]
}

p_values <- mclapply(1:1000, function(x) replicate(), mc.cores = 4)

mean(unlist(p_values) < 0.05)

# Run the power analysis simulation
p_values <- unlist(p_values)  # Convert list to vector

# Create histogram
ggplot(data.frame(p_values), aes(x = p_values)) +
  geom_histogram(binwidth = 0.05, fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Distribution of p-values from 1000 Simulations",
       x = "p-value",
       y = "Frequency") +
  theme_minimal()

```

### Third, we move to a CRT with baseline period
The baseline and follow-up measurements can be collected from the same participants (cohort design) or different participants (cross-sectional design), though the impact on the design effect depends on what approach is taken.
The key idea is to measure the same outcome at two different time points to reduce variance and improve statistical power.

ğ‘Œğ‘–ğ‘—ğ‘˜ = ğ›¼0 + ğ›¼1ğ‘˜ + ğ›¿0ğ‘ğ‘— + ğ›¿1ğ‘˜ğ‘ğ‘— + ğ‘ğ‘— + ğ‘ğ‘ğ‘—ğ‘˜ + ğ‘ ğ‘–ğ‘— + ğ‘ ğ‘ğ‘–ğ‘—ğ‘˜

where ğ‘Œğ‘–ğ‘—ğ‘˜ is a continuous outcome measure for individual ğ‘– in site ğ‘— and measurement ğ‘˜âˆˆ{0,1}. ğ‘˜=0 for baseline measurement, and ğ‘˜=1 for the follow-up. 
ğ‘ğ‘— is the treatment status of cluster ğ‘—, ğ‘ğ‘—âˆˆ{0,1}. 
ğ›¼0 is the mean outcome at baseline for participants in the control cluster. Baseline mean outcome in control clusters.
ğ›¼1 is the change from baseline to follow-up in the control arm. Change over time in the control arm (i.e., how much the outcome changes naturally without intervention).
ğ›¿0 is the difference at baseline between control and treatment arms (we would expect this to be 0 in a randomized trial)  
ğ›¿1 is the difference in the change from baseline to follow-up between the two arms. In a randomized trial, since ğ›¿0 should be close to 0, ğ›¿1 is the treatment effect. Treatment effect, i.e., the difference in change from baseline to follow-up between treatment and control arms.

The model has cluster-specific and individual-specific random effects. 
For both, there can be time-invariant effects and time-varying effects.
Cluster-level effects (between-site variation):
ğ‘ğ‘—âˆ¼ğ‘(0,ğœ2ğ‘) are time invariant site-specific effects. Intrinsic differences between clusters.
ğ‘ğ‘ğ‘—ğ‘˜ âˆ¼ğ‘(0,ğœ2ğ‘ğ‘) are the site-specific period (time varying) effects. Changes over time within a site.
Individual-level effects (within-site variation):
ğ‘ ğ‘–ğ‘—âˆ¼ğ‘(0,ğœ2ğ‘ ) are time invariant individual-level effects, stable individual characteristics.
ğ‘ ğ‘ğ‘–ğ‘—ğ‘˜âˆ¼ğ‘(0,ğœ2ğ‘ ğ‘) are the individual-level period (time varying) effects, measurement error or individual change over time.

Why Does This Help Reduce Sample Size?
1. Since each individual (or site) has two measurements, we can control for their baseline score when estimating the treatment effect.
2. This reduces residual variance, leading to a lower design effect and a smaller required sample size compared to a standard CRT.
3. The benefit depends on the intra-cluster correlation (ICC) and how much baseline values predict follow-up values.

#### Let's generate the data
```{r}
# Parameters:

# effect: The treatment effect to be used for the formula in the outcome variable.
# nsites: The number of sites (clusters) in the study.
# n: The number of participants at each site.
# s_c, s_cp, s_s, s_sp: These represent the variances of the respective random effects:
#  s_c: Variance of cluster-level (site-level) time-invariant random effects.
#  s_cp: Variance of cluster-level (site-level) time-varying random effects (due to different periods).
#  s_s: Variance of participant-level time-invariant random effects.
#  s_sp: Variance of participant-level time-varying random effects (due to periods).

crt_base <- function(effect, nsites, n, s_c, s_cp, s_s, s_sp) {
  # Variable c = cluster level, with variance s_c. It represents the time-invariant random effects at the cluster level.
  defC <- defData(varname = "c", formula = 0, variance = "..s_c")
  defC <- defData(defC, varname = "rx", formula = "1;1", dist = "trtAssign")
  # Cluster-Level Time-Varying Effects (defCP)
  # This defines time-varying cluster-level random effects c.p with variance s_cp. This effect captures the change over time within clusters, for example, the different responses between baseline and follow-up at each site.
  defCP <- defDataAdd(varname = "c.p", formula = 0, variance = "..s_cp")
  # This defines the individual-level random effects s with variance s_s. This represents individual individual variability that is constant across time.
  defS <- defDataAdd(varname = "s", formula = 0, variance = "..s_s")
  # The treatment effect (effect) for the interaction between the treatment indicator (rx) and the period (period), which gives the treatment effect over time.
  defSP <- defDataAdd(varname = "y",
    formula = "..effect * rx * period + c + c.p + s", 
    variance ="..s_sp")
  
  dc <- genData(nsites, defC, id = "site")
  
  # Add Periods to Cluster Data (dcp)
  # This adds two periods (baseline and follow-up) to each cluster and adds the time-varying random effects (c.p).
  dcp <- addPeriods(dc, 2, "site")
  dcp <- addColumns(defCP, dcp)
  dcp <- dcp[, .(site, period, c.p, timeID)]
  
  # Generate Individual-Level Data (ds)
  ds <- genCluster(dc, "site", n, "id")
  ds <- addColumns(defS, ds)
  
  # Add Periods to Individual-Level Data (dsp)
  # This adds two periods (baseline and follow-up) for each individual, creating the observational ID obsID.
  dsp <- addPeriods(ds, 2)
  setnames(dsp, "timeID", "obsID")
  
  setkey(dsp, site, period)
  setkey(dcp, site, period)
  
  # Merge Cluster-Level and Individual-Level Data (dd)
  dd <- merge(dsp, dcp)
  dd <- addColumns(defSP, dd)
  setkey(dd, site, id, period)
  
  dd[]
}

```

Design effect

In their paper, Teerenstra et al develop a design effect that takes into account the baseline measurement.

The correlation of two participant measurements in the same cluster and same time period is the ICC or ğœŒ, and is:

ğœŒ = (ğœ2ğ‘ + ğœ2ğ‘ğ‘) / (ğœ2ğ‘ + ğœ2ğ‘ğ‘ + ğœ2ğ‘  + ğœ2ğ‘ ğ‘)

In order to estimate the design effect, we need two more correlations. 
First, the correlation between baseline and follow-up random effects at the cluster level:

ğœŒğ‘ = ğœ2ğ‘/ (ğœ2ğ‘ + ğœ2ğ‘ğ‘)

Second, the correlation between baseline and follow-up random effects at the individual level:

ğœŒğ‘  = ğœ2ğ‘ / (ğœ2ğ‘ +ğœ2ğ‘ ğ‘)

A value ğ‘Ÿ is used to estimate the design effect, and is defined as:

ğ‘Ÿ= (ğ‘›ğœŒğœŒğ‘ + (1âˆ’ğœŒ)ğœŒğ‘ ) / (1 + (ğ‘›âˆ’1)ğœŒ)

If we are able to collect baseline measurements and our focus is on estimating ğ›¿1 from the model, the design effect is slightly modified from before:

(1+(ğ‘›âˆ’1)ğœŒ) * (2(1âˆ’ğ‘Ÿ))

-> Basically, adding the second part with rho. r: The correlation factor, which is a weighted combination of the cluster-level and individual-level correlations.
The first part of the formula is similar to the design effect formula used when baseline data isn't available. It adjusts for the clustering of individuals within sites (clusters), accounting for the intra-cluster correlation.
The second part of the formula adjusts for the reduction in variance due to the inclusion of baseline measurements in the analysis. The value of r reflects how strongly the baseline and follow-up measurements correlate at both the cluster and individual levels.
If the correlation between baseline and follow-up measurements is strong (i.e., high ps and high pc), this part of the formula will be small, implying a lower design effect (and thus a lower required sample size).

By collecting baseline measurements, the baseline-to-follow-up correlation reduces the variability in the outcome variable, which allows for more precise estimates of the treatment effect and smaller sample size requirements. Specifically, reducing noise and allowing you to detect smaller treatment effects with the same sample size.
By using the correlation between baseline and follow-up data, you can "shrink" the variability and make the study more efficient. This is crucial if the cost or effort of data collection is high, as it helps you achieve the same statistical power with a smaller sample.


#### Cross-section cohort design
We may not be able to collect two measurements for each participants at a site, but if we can collect measurements on two different cohorts, one at baseline before the intervention is implemented, and one cohort in a second period (either after the intervention has been implemented or not, depending on the randomization assignment of the cluster), we might be able to reduce the number of clusters.

In this case, ğœ2ğ‘ = 0 and ğœŒğ‘ = 0, so the general model reduces to:

ğ‘Œğ‘–ğ‘—ğ‘˜ = ğ›¼0 + ğ›¼1ğ‘˜ + ğ›¿0ğ‘ğ‘— + ğ›¿1ğ‘˜ğ‘ğ‘— + ğ‘ğ‘— + ğ‘ğ‘ğ‘—ğ‘˜ + ğ‘ ğ‘ğ‘–ğ‘—ğ‘˜

So, we can drop ğ‘ ğ‘–ğ‘— because there is no individual-level correlation (independent participants)

The parameters for this simulation are 
ğ›¿1= 2.4 (treatment effect, see above)
ğœ2ğ‘= 6.8 (Cluster-level variance)
ğœ2ğ‘ğ‘= 2.8 (Cluster-level period variance)
ğœ2ğ‘ ğ‘= 54.4 (Individual-level period variance) 
Total variance:ğœ2ğ‘ + ğœ2ğ‘ğ‘ + ğœ2ğ‘ ğ‘ = 6.8 + 2.8 + 54.4 = 64, as used previously.

```{r}
dd <- crt_base(effect = 2.4, nsites = 20, n = 30, s_c = 6.8, s_cp = 2.8, s_s = 0, s_sp = 54.4)

# Create a new column for period names (baseline and follow-up)
dd[, period_label := ifelse(period == 0, "Baseline", "Follow-up")]

# Create a violin plot, faceted by site
ggplot(dd, aes(x = period_label, y = y, fill = factor(rx))) + 
  geom_violin(trim = FALSE, alpha = 0.1) +  # Make the violin plot transparent
  geom_jitter(aes(color = factor(rx)), width = 0.1, alpha = 0.5) +  # Add individual data points
  geom_boxplot(width = 0.1, color = "black", alpha = 0.3, outlier.shape = NA) +  # Add boxplots
  facet_wrap(~site, ncol = 5) +  # 5 columns for 20 sites
  scale_fill_manual(values = c("blue", "red"), labels = c("Control", "Intervention")) +
  scale_color_manual(values = c("blue", "red")) +
  labs(title = "Violin Plot with Data Points and Boxplots by Site and Treatment Group", 
       x = "Measurement Time", y = "Outcome Value") +
  theme_minimal() + 
  theme(legend.position = "bottom", 
        strip.text = element_text(size = 8), 
        axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust text angles for clarity

```

#### Let's estimate the effect size
To estimate the effect size we fit a mixed effect model with cluster-specific effects only (both time invariant and time varying).
Treatment effect under "period*rx" !
Translating the Formula:
Fixed Effects:
Fixed effect for ğ›¼1ğ‘˜: This is represented by period in the model, which captures the difference between baseline and follow-up.
Fixed effect for treatment group (rx) ğ›¿0ğ‘ğ‘— : This is represented by rx and captures whether the site received treatment or control.
Interaction of time and treatment (period * rx) ğ›¿1ğ‘˜ğ‘ğ‘— : This interaction term captures how the treatment effect differs between baseline and follow-up.
Random Effects:
(1 | site): ğ‘ğ‘— : A random intercept for each site. This accounts for the variability between sites but assumes the variability is constant across time (since it's not tied to the period).
(1 | timeID:site):ğ‘ğ‘ğ‘—ğ‘˜: A random intercept for each site in each time period (baseline and follow-up). This accounts for the site-specific changes over timeâ€”it captures how variability between sites changes from baseline to follow-up.
s pij is not included in your current model for the cross-sectional design because you are assuming independence between individuals at baseline and follow-up.
```{r}
dd <- crt_base(effect = 2.4, nsites = 200, n = 100, s_c=6.8, s_cp=2.8, s_s=0, s_sp=54.4)

fit3 <- lmer(y ~ period * rx + (1|timeID:site) + (1 | site), data = dd)
tbl_regression(fit3, tidy_fun = broom.mixed::tidy)  %>% 
  modify_footnote(ci ~ NA, abbreviation = TRUE)
```

#### Let's update the Design Effect
```{r}
# Based on the variance assumptions, we can update our design effect:
s_c <- 6.8
s_cp <- 2.8
s_s <- 0
s_sp <- 54.4

rho <- (s_c + s_cp)/(s_c + s_cp + s_s + s_sp)
rho_c <- s_c/(s_c + s_cp)
rho_s <- s_s/(s_s + s_sp)

n <- 30

r <- (n * rho * rho_c + (1-rho) * rho_s) / (1 + (n-1) * rho)

# The design effect for the CRT without any baseline measurement was 5.35. With the two-cohort design, the design effect is reduced slightly:
(des_effect <- (1 + (n - 1) * rho) * 2 * (1 - r))
## [1] 4.3

# and thus leaves us with:
des_effect * 350 / n
## [1] 50

# The desired number of sites is over 50, so rounding up to the next even number gives us 52
```

#### Let's confirm the power
After calculating the design effect, we run a simulation to confirm the statistical power based on the specified number of sites (52) and participants per site (30). The goal is to check whether the p-value for the treatment effect (period*rx) is statistically significant in at least 80% of simulations.
```{r}
replicate <- function() {
  dd <- crt_base(2.4, 52, 30, s_c = 6.8, s_cp = 2.8, s_s = 0, s_sp = 54.4)
  fit3 <- lmer(y ~ period * rx + (1|timeID:site) + (1 | site), data = dd)
  
  coef(summary(fit3))["period:rx", "Pr(>|t|)"]
}

p_values <- mclapply(1:1000, function(x) replicate(), mc.cores = 4)

mean(unlist(p_values) < 0.05)

```

#### Closed cohort design
We can reduce the number of clusters further if instead of measuring one cohort prior to the intervention and another after the intervention, we measure a single cohort twice - once at baseline and once at follow-up. Now we use the full model that decomposes the participant level variance into a time invariant effect (ğ‘ğ‘—) and a time varying effect (ğ‘ğ‘ğ‘—ğ‘˜):

ğ‘Œğ‘–ğ‘—ğ‘˜ = ğ›¼0 + ğ›¼1ğ‘˜ + ğ›¿0ğ‘ğ‘— + ğ›¿1ğ‘˜ğ‘ğ‘— + ğ‘ğ‘— + ğ‘ğ‘ğ‘—ğ‘˜ + ğ‘ ğ‘–ğ‘— + ğ‘ ğ‘ğ‘–ğ‘—ğ‘˜

#### Let's generate the data
The parameters for this simulation are 
ğ›¿1 = 2.4 (treatment effect, see above)
ğœ2ğ‘ = 6.8 (Cluster-level variance)
ğœ2ğ‘ğ‘ = 2.8 (Cluster-level period variance)
ğœğ‘  = 38 (Individual-level variance) 
ğœ2ğ‘ ğ‘ = 16.4 (Individual-level period variance) 
Total variance = 64, as used previously.
```{r}
dd <- crt_base(effect=2.4, nsites=20, n=30, s_c=6.8, s_cp=2.8, s_s=38, s_sp=16.4)

# Visualizing the data (2 time points: baseline and follow-up)
ggplot(dd, aes(x = period, y = y, group = id, color = as.factor(rx))) +
  geom_line() +
  geom_point(aes(shape = as.factor(rx)), size = 3) +
  facet_wrap(~site, ncol = 5) +
  scale_color_manual(values = c("blue", "red")) +
  labs(title = "Repeated Measurements: Outcome by Site and Time",
       x = "Time Period (0 = Baseline, 1 = Follow-up)",
       y = "Outcome (y)",
       color = "Treatment",
       shape = "Treatment") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

#### Let's estimate the effect size
The mixed effect model includes cluster-specific effects only (both time invariant and time varying), as well as subject level effects. 
```{r}
dd <- crt_base(effect = 2.4, nsites = 200, n = 100, 
  s_c = 6.8, s_cp = 2.8, s_s = 38, s_sp = 16.4)

fit4 <- lmer(y ~ period*rx + (1 | id:site) + (1|timeID:site) + (1 | site), data = dd)
tbl_regression(fit4, tidy_fun = broom.mixed::tidy)  %>% 
  modify_footnote(ci ~ NA, abbreviation = TRUE)
```

#### Let's update the Design Effect
```{r}
# Based on the variance assumptions, we can update our design effect a second time:
s_c <- 6.8
s_cp <- 2.8
s_s <- 38
s_sp <- 16.4

rho <- (s_c + s_cp)/(s_c + s_cp + s_s + s_sp)
rho_c <- s_c/(s_c + s_cp)
rho_s <- s_s/(s_s + s_sp)

n <- 30

r <- (n * rho * rho_c + (1-rho) * rho_s) / (1 + (n-1) * rho)

# And again, the design effect (and sample size requirement) is reduced:
(des_effect <- (1 + (n - 1) * rho) * 2 * (1 - r))
## [1] 3.1
des_effect * 350 / n
## [1] 37

# The desired number of sites is over 36, so I will round up to 38
```

#### Let's confirm the power
After calculating the design effect, we run a simulation to confirm the statistical power based on the specified number of sites (52) and participants per site (30). The goal is to check whether the p-value for the treatment effect (period*rx) is statistically significant in at least 80% of simulations.
```{r}
replicate <- function() {
  dd <- crt_base(2.4, 38, 30, s_c = 6.8, s_cp = 2.8, s_s = 38, s_sp = 16.4)
  fit4 <-  lmer(y ~ period*rx + (1 | id:site) + (1|timeID:site) + (1 | site), data = dd)
  
  coef(summary(fit4))["period:rx", "Pr(>|t|)"]
}

p_values <- mclapply(1:50, function(x) replicate(), mc.cores = 4) # increase once more computational capacity

mean(unlist(p_values) < 0.05)
## [1] 0.79
```

#### Repeated Measurements - ANCOVA Model
We may be able to reduce the number of clusters even further by changing the model so that we are comparing follow-up outcomes of the two treatment arms (as opposed to measuring the differences in changes as we just did). This model is

ğ‘Œğ‘–ğ‘—1 = ğ›¼0 + ğ›¾ğ‘Œğ‘–ğ‘—0 + ğ›¿ğ‘ğ‘— + ğ‘ğ‘— + ğ‘ ğ‘–ğ‘—

ğ‘Œğ‘–ğ‘—0: Baseline outcome for the same individual
ğ›¾ = Adjustment coefficient for baseline values (typically close to 1 if baseline and follow-up outcomes are highly correlated).
ğ‘ğ‘—: Treatment assignment (0 = control, 1 = intervention). 
ğ›¿:Treatment effect of interest.
ğ‘ğ‘—: Cluster-level random effect.
ğ‘ ğ‘–ğ‘—: Subject-level random effect.


Even though the estimation model has changed, I am using the exact same data generation process as before, with the same effect size and variance assumptions:
```{r}
dd <- crt_base(effect = 2.4, nsites = 200, n = 100, 
  s_c = 6.8, s_cp = 2.8, s_s = 38, s_sp = 16.4)

dobs <- dd[, .(site, rx, id, period, timeID, y)]
dobs <- dcast(dobs, site + rx + id ~ period, value.var = "y")

fit5 <- lmer(`1` ~ `0` + rx + (1 | site), data = dobs)
tbl_regression(fit5, tidy_fun = broom.mixed::tidy)  %>% 
  modify_footnote(ci ~ NA, abbreviation = TRUE)
```

#### Let's update the Design Effect
Teerenstra et al derived an alternative design effect that is specific to the ANCOVA model:

(1+(ğ‘›âˆ’1)ğœŒ)(1âˆ’ğ‘Ÿ2)

where ğ‘Ÿ is the same as before. Since (1âˆ’ğ‘Ÿ2) < 2(1âˆ’ğ‘Ÿ), 0â‰¤ğ‘Ÿ<1, this will be a reduction from the earlier model.
```{r}
(des_effect <- (1 + (n - 1) * rho) * (1 - r^2))
## [1] 2.7
des_effect * 350 / n
## [1] 31
```

#### Let's confirm the power
```{r}
replicate <- function() {
  
  dd <- crt_base(2.4, 32, 30, s_c = 6.8, s_cp = 2.8, s_s = 38, s_sp = 16.4)
  dobs <- dd[, .(site, rx, id, period, timeID, y)]
  dobs <- dcast(dobs, site + rx + id ~ period, value.var = "y")

  fit5 <- lmer(`1` ~ `0` + rx + (1 | site), data = dobs)
  coef(summary(fit5))["rx", "Pr(>|t|)"]
}

p_values <- mclapply(1:1000, function(x) replicate(), mc.cores = 4)

mean(unlist(p_values) < 0.05)
## [1] 0.78
```


## Simulation | Binary outcome
### First, start with a simple individual randomized trial
Requirements:
power: 80%
alpha: 5%
Baseline uptake PrEP: 30% (see survey data)
Increase in uptake through intervention: 10%

```{r}
# First, calculate the sample size for this individual RCT, using pwr, two-sided (effect could go either way)
alpha <- 0.05 # alpha level
power <- 0.80 # power
p_cont <- 0.30 # Proportion of outcome in the control group
p_int <- 0.40 # Proportion of outcome in the intervention group

power.prop.test(p1 = p_cont, p2 = p_int, power = power, sig.level = alpha)
ss_ind_1arm <- pwr.2p.test(h = ES.h(p_int, p_cont), 
                           sig.level = alpha, 
                           power = power)
ss_ind <- ss_ind_1arm$n * 2
cat("Total sample size individual RCT, pwr:", round(ss_ind, 0))

## As a comparison, use "manual" formula instead of pwr package
Z_alpha_half <- qnorm(1 - alpha / 2) # translate into Z-distribution -> equals 0.975 (95% CI)
Z_beta <- qnorm(power)
ss_ind_1arm_man <- ((Z_alpha_half + Z_beta)^2 * (p_int * (1 - p_int) + p_cont * (1 - p_cont))) / (p_int - p_cont)^2

ss_ind_man <- ss_ind_1arm_man * 2
# cat("Total sample size individual RCT, manual:", ss_ind_man) # total sample size
# => very similar result, use pwr result going forward

cat("Total sample size individual RCT, manual:", round(ss_ind_man, 0))

```

#### Let's generate the data
```{r}
simple_rct_binary_probs <- function(N, p0, p1) {
  
  # Step 1: Assign treatment randomly (1:1 allocation)
  defS <- defData(varname = "rx", formula = "1;1", dist = "trtAssign")
  dd <- genData(N, defS)
  
  # Step 2: Assign binary outcome (success/failure) for each group using Bernoulli distribution
  # Control group (rx == 0)
  dd[rx == 0, y := rbinom(sum(rx == 0), 1, p0)]
  # Treatment group (rx == 1)
  dd[rx == 1, y := rbinom(sum(rx == 1), 1, p1)]
  
  return(dd)
}

# Generate data
dd_binary <- simple_rct_binary_probs(1000, p0 = 0.3, p1 = 0.4)

# Check the proportions
prop_control <- mean(dd_binary$y[dd_binary$rx == 0])  
prop_treatment <- mean(dd_binary$y[dd_binary$rx == 1])

cat("Observed control success rate:", prop_control, "\n")
cat("Observed treatment success rate:", prop_treatment, "\n")

# Visualizing Binary Outcome Distribution
ggplot(dd_binary, aes(x = factor(rx), fill = factor(y))) + 
  geom_bar(position = "fill", color = "black") +  # Stacked bar plot showing proportion of successes (y=1)
  scale_fill_manual(values = c("gray", "blue"), labels = c("No PrEP", "PrEP Uptake")) +
  labs(x = "Treatment Group", y = "Proportion", fill = "Outcome") +
  theme_minimal() +
  ggtitle("Proportion of PrEP Uptake by Treatment Group")
```

#### Let's estimate the effect size
```{r}
fit_logit <- glm(y ~ rx, data = dd_binary, family = binomial)
tbl_regression(fit_logit) %>% 
  modify_footnote(ci ~ NA, abbreviation = TRUE)

# Get the exp coefficient (i.e. OR) and exp confidence intervals
# coef_summary <- exp(summary(fit_logit)$coefficients)

```

#### Let's confirm the power
```{r}
replicate <- function() {
  dd_binary <- simple_rct_binary_probs(710, p0 = 0.3, p1 = 0.4)
  fit_logit <- glm(y ~ rx, data = dd_binary, family = binomial)
  
  coef(summary(fit_logit))["rx", "Pr(>|z|)"]
}

p_values <- mclapply(1:1000, function(x) replicate(), mc.cores = 4)

# Estimated power based on 1000 replications.
mean(unlist(p_values) < 0.05)
```

### Second, now move to a cluster randomized trial, but same context
#### Sample size calculation
ICC: 0.15 (this is rather conservative, according to guidance from CDC guidance re ICC for PrEP uptake as outcome [https://www.cdc.gov/hiv/pdf/research/interventionresearch/compendium/prep/PrEP_Chapter_EBI_Criteria.pdf] and see data from SEARCH [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9169331/], and realistic for a behavioral intervention)
Cluster size: 50
Since individuals in the same cluster are correlated, we need to adjust the sample size using the design effect.
Design Effect = 1+(nâˆ’1)Ï; where, n=50 (number of individuals per site), Ï = 0.15
=> Design effect = 8.35
```{r}
cluster_size <- 50  # Average cluster size, i.e. participants per cluster
icc <- 0.15
deff_c <- 1 + (cluster_size - 1) * icc # design effect due to cluster randomization

ss_crt_standard <- ss_ind * deff_c
cat("Total sample size CRT, standard:", ss_crt_standard) # total sample size for a standard CRT
n_clus_crt_standard <- ss_crt_standard / cluster_size
cat("Total N clusters CRT, standard:", n_clus_crt_standard) # total number of clusters for a standard CRT (divide by arm or sequence/steps if SWCRT)
```

#### Let's generate the data
Use a logistic model: Instead of y being normally distributed, we model the log-odds (logit) of success.
Convert the linear predictor into a probability: Use the inverse logit (logistic function):
P(Y ij = 1) = e Î·ij / 1 + e Î·ij
Î·ij = cj + Î²â‹…rxj
Simulate binary outcomes: Draw y from a Bernoulli distribution based on the computed probabilities.

Since youâ€™ve estimated an intraclass correlation coefficient (ICC) of 0.10 for PrEP uptake, we can now determine reasonable variance parameters for your binary outcome model.
In logistic models, the ICC is the proportion of total variance explained by between-site variation (i.e., clustering).

ICC = Between-site variance / Total variance

The total variance in a logistic model is fixed at Ï€^2 / 3 = 3.29 for the residual level (individual variation).
The between-site variance (Ïƒ^2c) is what we need to determine.

The ICC formula for a logistic model is:

ICC = Ïƒ^2c / Ïƒ^2c + (Ï€^2 / 3)

If thereâ€™s additional within-site variation over time, we include Ïƒ^2cp.

Typically, this is a fraction of Ïƒ^2c, e.g., half the site-level variance.
```{r}
simple_crt_binary <- function(nsites, n, p_cont, p_int, ICC) {
  # Convert the proportions to log-odds
  logit_cont <- log(p_cont / (1 - p_cont))  # Log-odds for control group
  logit_int <- log(p_int / (1 - p_int))    # Log-odds for intervention group
  
  # Calculate the treatment effect (log-odds difference)
  log_OR <- logit_int - logit_cont  # Difference in log-odds
  
  # Calculate the between-cluster variance (Ïƒ^2c) from ICC
  sigma2_c <- ICC * (pi^2 / 3) / (1 - ICC)  # Between-site variance
  cat("Between-site variance (Ïƒ^2c):", sigma2_c, "\n")
  
  # Treatment assignment (1:1 randomization)
  defC <- defData(varname = "rx", formula = "1;1", dist = "trtAssign")
  
  # Define Site-Level Random Effect (c) with variance Ïƒ^2c
  defC <- defData(defC, varname = "c", formula = "0", variance = sigma2_c, dist = "normal")
  
  # Define Individual-Level Outcome (logit model)
  defS <- defDataAdd(varname = "y", 
                     formula = paste0("c + ", logit_cont, " + ", log_OR, " * rx"), 
                     dist = "binary", link = "logit")
  
  # Generate site-level data
  dc <- genData(nsites, defC, id = "site")
  
  # Generate individual-level data (n participants per site)
  dd <- genCluster(dc, "site", n, "id")
  dd <- addColumns(defS, dd)
  
  return(dd)
}

# Example: Simulating the data
p_cont <- 0.30  # Proportion of outcome in the control group
p_int <- 0.40   # Proportion of outcome in the intervention group
ICC <- 0.15     # Intraclass correlation coefficient (low ICC)
n <- 50          # Number of participants per site
nsites <- 118    # Number of sites (clusters)

# Simulate the data
dd_binary <- simple_crt_binary(nsites = nsites, n = n, p_cont = p_cont, p_int = p_int, ICC = ICC)

# Calculate the success rate for each treatment group
dd_binary_summary <- dd_binary %>%
  group_by(rx) %>%
  summarise(success_rate = mean(y))

# View the summary
print(dd_binary_summary)

# Create violin plot with sites on x-axis and colors for treatment/control
ggplot(dd_binary, aes(x = factor(site), y = y, fill = factor(rx))) +
  geom_violin(trim = FALSE, alpha = 0.5) +  # Violin plot with transparency
  geom_jitter(shape = 16, position = position_jitter(0.2), alpha = 0.4, size = 2) +  # Scatter individual points
  scale_fill_manual(values = c("blue", "red"), labels = c("Control", "Treatment")) +
  labs(x = "Site (Cluster)", y = "Outcome (y)", title = "Binary Outcome Distribution by Site") +
  theme_minimal() +
  theme(legend.title = element_blank(), axis.text.x = element_text(angle = 90, hjust = 1))

# Check the distribution of treatment and control groups within each site
addmargins(table(dd_binary$rx, dd_binary$site))

# Define probabilities
p_cont <- 0.30  # Proportion of outcome in the control group
p_int <- 0.40   # Proportion of outcome in the intervention group

# Calculate log-odds for control and intervention groups
logit_cont <- log(p_cont / (1 - p_cont))  # Log-odds for control group
logit_int <- log(p_int / (1 - p_int))    # Log-odds for intervention group

# Calculate the treatment effect as the difference in log-odds
log_OR <- logit_int - logit_cont  # Difference in log-odds (treatment effect)

# Calculate the odds ratio (OR)
OR <- (p_int / (1 - p_int)) / (p_cont / (1 - p_cont))  # Odds ratio

# Calculate the log of the odds ratio (log-OR)
log_OR_from_OR <- log(OR)  # Log of odds ratio

# Print the results
cat("Log-odds for Control Group:", logit_cont, "\n")
cat("Log-odds for Intervention Group:", logit_int, "\n")
cat("Treatment Effect (Log-Odds Difference):", log_OR, "\n")
cat("Odds Ratio (OR):", OR, "\n")
cat("Log-Odds Ratio (log(OR)):", log_OR_from_OR, "\n") # same as treatment effect, but both on log

```

#### Let's estimate the effect size
A logistic mixed effects model with random intercept is used to estimate the effect size.
```{r}
dd_binary <- simple_crt_binary(nsites = nsites, n = n, p_cont = p_cont, p_int = p_int, ICC = ICC)

# Fit a logistic mixed-effects model with random intercepts for site
fit_logistic <- glmer(y ~ rx + (1 | site), data = dd_binary, family = binomial)

tbl_regression(fit_logistic, tidy_fun = broom.mixed::tidy)  %>% 
  modify_footnote(ci ~ NA, abbreviation = TRUE)

# Get the exp coefficient (i.e. OR) and exp confidence intervals
coef_summary <- exp(summary(fit_logistic)$coefficients)

```

#### Let's confirm the power
```{r}
replicate <- function() {
  dd_binary <- simple_crt_binary(nsites = nsites, n = n, p_cont = p_cont, p_int = p_int, ICC = ICC)
  fit_logistic <- glmer(y ~ rx + (1 | site), data = dd_binary, family = binomial)
  
  coef(summary(fit_logistic))["rx", "Pr(>|z|)"]
}

p_values <- mclapply(1:100, function(x) replicate(), mc.cores = 4)

mean(unlist(p_values) < 0.05)

p_values <- unlist(p_values)  # Convert list to vector

# Create histogram
ggplot(data.frame(p_values), aes(x = p_values)) +
  geom_histogram(binwidth = 0.05, fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Distribution of p-values from xxx Simulations",
       x = "p-value",
       y = "Frequency") +
  theme_minimal()

```




